# AI知识笔记



## 一、了解GPT

​		ChatGPT能说会到能、写代码，修Bug，还能理解docker容器的使用等。

​		目前看ChatGPT的水平很高，我们通过搜索引擎也能够获取得到，但是搜索精准程度还是差得很多并不是第一时间就能够找到自己想要的答案。

​		ChatGPT在代码理解、编写代码方面视乎不太“稳定”，重复问同一个问题写出的代码很大可能是不一致的，输出代码的可用水平很大程度取决于提问者的提问水平。

  当然目前的ChatGPT回答得怎样关键还得看提问者问题的的**信息量**程度，问题越明确回答的也就会越精准，问题越模糊回答的可能就不是你想要的。

> GPT擅长回答问题，但是只能回答它以前被训练过的问题，如果是没有训练过的数据，比如一些私有数据或者最新的数据该怎么办呢？
>
> 这种情况下通常有两种办法，一种是微调(fine-tuning)，一种是嵌入(embedding)。
>
> 微调就是在大模型的数据基础上做二次训练，事先准备好一批prompt-complition(类似于问答Q&A)的数据，生成新的模型，这个模型将会包含微调后的数据。
>
> 而嵌入则是每次向ChatGPT发送消息(prompt)的时候，把你自己数据结果带上。



## 二、GPT使用举例

### 1.脑筋急转弯测试

<img src="https://raw.githubusercontent.com/zyx3721/Picbed/main/blog-images/2024/06/03/6fe6184d6e04f5cdd5f30e70e25792a9-image-20240603113033289-f5b3f6.png" alt="image-20240603113033289" style="zoom:50%;" />

### 2.生成一张图片测试

生成一张深圳的图片，要让人一眼认出图片就是深圳：

<img src="https://raw.githubusercontent.com/zyx3721/Picbed/main/blog-images/2024/06/03/bd7e0c71234dae61470cc6dc0b86335e-image-20240603113102625-bc6000.png" alt="image-20240603113102625" style="zoom:50%;" />

![image-20240603113120747](https://raw.githubusercontent.com/zyx3721/Picbed/main/blog-images/2024/06/03/908ba60803adbc7a2e7b686e6df81b6c-image-20240603113120747-e5dd88.png)

图片放大效果：

<img src="https://raw.githubusercontent.com/joshzhong66/Pibced/main/blog-images/2024/05/11/eabb77e4dd67f2e5528ffed90f5f4934-file-JhpO1UXYPkdyoXlIKTuY4iFW-494f75" alt="深圳的动态形象，突出显示被现代摩天大楼群包围的平安金融中心。天空晴朗，有几朵云，前景中可见深圳湾大桥，凸显了这座城市充满活力的现代气息。" style="zoom:67%;" />

对图片进行解释：

<img src="https://raw.githubusercontent.com/zyx3721/Picbed/main/blog-images/2024/06/03/43c4008b572c96c92008ae4e657795f4-image-20240603113147412-db6cdd.png" alt="image-20240603113147412" style="zoom:67%;" />

```bash
当然可以！以下是一首用现代诗歌形式描述这张深圳图片的作品：

在蓝天之下，
高耸入云的塔楼，
如梦想的手臂，
直指无垠的苍穹。

平安金融中心，
傲然屹立，
现代化的诗篇，
在玻璃与钢铁中谱写。

四周的高楼，
如星辰环绕，
在城市的脉搏中，
诉说着繁华的节奏。

深圳湾大桥，
如银色的丝带，
连接着远方的梦想，
与这片繁忙的土地。

在这里，
自然与人文交织，
蓝天白云映衬下，
城市的脉络清晰可见。

这是一幅画，
更是一首诗，
在现代与自然的交响中，
诉说着深圳的故事。

希望您喜欢这首诗歌！如果有任何调整或其他需求，请告诉我。
```

厦门的情况介绍：

<img src="https://raw.githubusercontent.com/zyx3721/Picbed/main/blog-images/2024/06/03/cf7bc72a3e0ec3b03e99ab73654f77a8-image-20240603113214487-b978b5.png" alt="image-20240603113214487" style="zoom:50%;" />



## 三、大模型语言概念的了解

​		大语言模型也只是将用户提供的大规模数据集训练而来，也并非万能的什么都知道，特别是一些小众知识、内部数据或私密的个人数据等，此时ChatGLM3肯定会胡乱回答就是ChatGPT4也不一定能给出满意回答。

​		不少公司、个人都有自己的知识库或日志等此时如有可将这些数据以某种方式挂在大模型上此时在知识库存在的知识可从中作答，不属于知识库中的内容还是按照大模型原来方式生成，精准度会高不少。

​		知识库的内容包括文本、PDF、图片、视频、网页等等。



## 四、本地知识库实现方式

​		基于LLM的本地私有化知识库实现主要分为两种：

- **模型训练微调：** 将知识库的内容整理成训练数据集，拿这些整理好的数据集来训练该模型，最终让模型“学会”该知识库的内容，至于效果如何很大程度取决于该数据集的质量和训练的调参，这种方式较复杂、门槛高；
- **外挂知识库：** 在向模型提问时提供一些知识库中的内容让它在其中找到正确的答案，外挂的形式门槛相对低一点大部分的工作主要是文档的处理：加载、切分、向量化、持久化、相识度对比等以及Prompt编写；

​		外挂知识库的最终效果如何取决于：文本分割、Embedding模型、向量库、LLM模型、知识库的预处理、调参等等。

​		**文档处理：**

  这里只涉及到文本的处理，比较好的做法是从知识库中挑选出问题以及相关的内容，这样准确的相对比较高，本文只是简单粗暴的把所有数据都丢给它让它去学习处理。



## 五、知识库相关概念

​		**加载文件：** 加载知识库中的文本。

  **文本分割（TextSplitter）：** 按一定规则将文本分割，具体参数有： separator:分隔符、chunk_size: 文本块长度、chunk_overlap: 文本块之间重叠的长度、length_function：计算长度的方法； 文本块长度选择可能会对文本分割效果右较大的影响。

  **文本向量化：** 将文本转换为向量，文本向量化后用于后续存储、计算相识度、检索相关文本等。

  **文本内嵌（Embedding）：** 将离散的符号或对象表示为连续的向量空间中的点；文本嵌入可用于以下功能：搜索、聚类、推荐、异常检测、多样本测量、分类；此处主要是用于查询的嵌入向量（问题向量化后）与每个文档的嵌入向量之间的余弦相似度，并返回得分最高的文档。Embedding开源模型模型有：Text2vec、Ernie-3.0、M3E等。

  **持久化：** 将向量化的数值存储到向量数据库方便后续直接使用，向量数据库有Chroma、Qdrant等。



## 六、知识库的意义

​		企业建设属于自己的知识库，可以降本增效，无论企业大小，都可以开始着手尝试应用！

​		如果还没有头绪着手开始这件事，以下方式可以参考：

- 指定1名人员作为知识库管理员，并安排其完成基于FastGPT的知识库问答系统搭建，功能使用培训；
- 各个部门指定1名人员并分配后台账号，负责录入并调校本部门知识档案的输出（包含但不限于规章制度、工作流程、优秀案例、企业文化、宣传资料、产品资料、技术文件等等），并定期维护和更新本部门资料；
- 由知识库管理员基于知识库创建不同功能的应用，如新员工AI培训师、AI财务助理、AI客服等等，还可以通过知识库API继续深度开发知识库相关应用程序。

​		FastGPT的出现，从一定意义上来说更加具体了知识库的概念，降低了知识库的入门标准，拉低了成本。



## 七、什么是RAG？

​		由于LLM回答问题存在一定的概率，且很多敏感、内部私有知识LLM也没有见过所以再一些问题的回答上任何大语言模型也不是什么都知道，这时候要求私有域数据LLM也能准确做出应答就需要对模型进行微调或外挂知识库，这里的外挂知识库就是Retrieval Augmented Generation(检索争增强生成)也就是RAG的全称。

​		在向LLM模型提问时根据提问文本从知识库中查找提问问题相关的内容**（余弦相似度）**，并将其作为问题的上下文传递给LLM模型，此时大语言模型会根据所问的问题与所提供的从知识库所得到的上下文回答该问题，为了防止LLM胡说八道，通常会在Prompt中提示LLM只从上下文中获取信息回答如上下文无法得到结果则回答不知道。

​		RAG包含两个主要流程，向量检索、响应生成。

​		**RAG流程图：**

​		最基本的RAG 流程为用户Query问题，RAG应用在向量库查询出Context，然后将Query与 Context喂到LLM，LLM生成满足Query的Response答案。

​		从这里可以存在三元组： **Query、Context 和 Response**，如要想进一步提升RAG的性能还可在Context(上下文)上做文章，目前得到的Context很可能与Query(问题)之间的相关度不高，通过**Rerank**可以对Context进行评分排序，去掉分数较低相关度较低的数据从而避免其对上下文信息的污染。下图为基本的RAG流程图；

<img src="https://raw.githubusercontent.com/zyx3721/Picbed/main/blog-images/2024/06/03/021f672bb173acf0281e926dc34df40e-image-20240603113535092-e0f798.png" alt="image-20240603113535092" style="zoom:67%;" />

 在未引入Rerank时可以看到从向量库中得到的上下文中包含了大量的问题相关度较低的信息，造成不相干信息对上下文的干扰。



## 八、Rerank模型

​		Rerank模型使用bge-reranker-base模型和embedding模型不同，Reranker模型使用问题和文档作为输入，直接输出相似度而不是embedding。

​		Reranker是基于交叉熵损失进行优化的相关性得分不受特定范围的限制，bge-reranker-base适用于中英文的双语场景。如果有更多语言需求可使用最新发布的BGE Re-Ranker v2.0模型。

​		Rerank模型可以在不牺牲准确性的情况下加速LLM的查询并可能提高准确率，Rerank通过从上下文中删除分数较低(相关性较低)的数据，Rerank通过重新排序上下文得以实现。下图为引入Rerank后的RAG流程图。

<img src="https://raw.githubusercontent.com/zyx3721/Picbed/main/blog-images/2024/06/03/5be27d43166c164e5be706b04e902fd5-image-20240603113608428-057c5e.png" alt="image-20240603113608428" style="zoom: 67%;" />



​		引入Rerank模型后的retrieval引擎能够去除上下文不相关的污染数据、提供更精准的上下文信息。重排后(Rerank)精准的上下文不仅可减少了token的使用量进而还可能提高LLM推理速度与准确率。



## 九、文本内嵌选择

​		**Embeddings模型（嵌入模型）**的选择至关重要，会影响到向量库检索的精度，从向量库检索的到文本内容如和提问文本关联度很低就会导致RAG应用质量降低。

​		比如中文知识文本肯定不能选择纯英文Embeddings模型作为文本嵌入模型，在生产环境也尽量不用参数规模较小的模型如ernie-3.0-nano-zh，可使用**ERNIE 3.0-Base 、text2vec-base-chinese**等规模相对大点的模型；

​		**目前使用VPN知识库测试对比：**

- **M3E+GPT3.5 Trubo**
- **M3E+Qwen14:B**



## 十、FastGPT 知识库完整逻辑

​		在构建知识库前，来了解下 FastGPT 是如何进行知识库检索的。首先了解几个基本概念：

- 向量：将人类直观的语言（文字、图片、视频等）转成计算机可识别的语言（数组）。
- 向量相似度：两个向量之间可以进行计算，得到一个相似度，即代表：两个语言相似的程度。
- 语言大模型的一些特点：上下文理解、总结和推理。

​		结合上述 3 个概念，便有了 “向量搜索 + 大模型 = 知识库问答” 的公式。下图是 FastGPT V3 中知识库问答功能的完整逻辑：

![image-20240603113702817](https://raw.githubusercontent.com/zyx3721/Picbed/main/blog-images/2024/06/03/bd5644c0bf6bcb5bfacb8e6bb08423c7-image-20240603113702817-1213f1.png)
